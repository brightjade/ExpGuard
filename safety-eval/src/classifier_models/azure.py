import os
import json
import requests
from typing import List, Dict
from dotenv import load_dotenv
from src.classifier_models.base import (
    SafetyClassifierBase,
    SafetyClassifierOutput,
    PromptHarmfulness,
    ResponseHarmfulness
)

# Load environment variables
load_dotenv()

class AzureContentSafetyClassifier(SafetyClassifierBase):
    def __init__(self, batch_size: int = 1, threshold: int = 4, save_path: str = "./azure_results.json", **kwargs):
        super().__init__(batch_size, **kwargs)
        self.threshold = threshold
        self.api_key = os.environ.get("AZURE_CONTENT_SAFETY_KEY")
        self.endpoint = os.environ.get("AZURE_CONTENT_SAFETY_ENDPOINT")
        self.api_url = f"{self.endpoint}/contentsafety/text:analyze?api-version=2024-09-01"
        self.save_path = save_path
        self.processed_data = self.load_partial_results()

    def get_required_input_fields(self) -> List[str]:
        return []

    def get_optional_input_fields(self) -> List[str]:
        return ["prompt", "response"]

    def get_output_fields(self) -> List[str]:
        return ["prompt_harmfulness", "response_harmfulness"]

    def _analyze_text(self, text: str) -> Dict:
        if not text:
            return {}

        headers = {
            "Ocp-Apim-Subscription-Key": self.api_key,
            "Content-Type": "application/json"
        }
        data = {"text": text}

        try:
            response = requests.post(self.api_url, headers=headers, json=data)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"âŒ Network error occurred: {e}")
            return {}

    def _parse_response(self, response: Dict) -> Dict[str, int]:
        category_mapping = {
            "Hate": "hate",
            "Violence": "violence",
            "SelfHarm": "selfHarm",
            "Sexual": "sexual"
        }

        scores = {value: 0 for value in category_mapping.values()}

        for item in response.get("categoriesAnalysis", []):
            category = item.get("category", None)
            severity = item.get("severity", 0)

            if category in category_mapping:
                mapped_category = category_mapping[category]
                scores[mapped_category] = severity

        return scores

    def load_partial_results(self) -> List[Dict]:
        if os.path.exists(self.save_path):
            try:
                with open(self.save_path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError):
                print("âš ï¸ Saved file is corrupted. Creating a new one.")
        return []


    def _classify_batch(
            self,
            batch: List[Dict[str, str]]
    ) -> List[SafetyClassifierOutput]:
        assert any("prompt" in item or "response" in item for item in batch), "Azure Content Safety classifier requires 'prompt' and/or 'response' field."

        results = [SafetyClassifierOutput(metadata={}) for _ in batch]

        try:
            for i, item in enumerate(batch):
                if item in self.processed_data:
                    print(f"ğŸ”„ Skipping already processed data: {item}")
                    results[i] = self.processed_data[item]
                    continue

                prompt_scores, response_scores = {}, {}

                if "prompt" in item and item["prompt"] is not None:
                    prompt_response = self._analyze_text(item["prompt"])
                    prompt_scores = self._parse_response(prompt_response)
                    prompt_harmful = any(score >= self.threshold for score in prompt_scores.values())
                    results[i].prompt_harmfulness = PromptHarmfulness.HARMFUL if prompt_harmful else PromptHarmfulness.UNHARMFUL
                    results[i].metadata["prompt_scores"] = prompt_scores

                if "response" in item and item["response"] is not None:
                    response_response = self._analyze_text(item["response"])
                    response_scores = self._parse_response(response_response)
                    response_harmful = any(score >= self.threshold for score in response_scores.values())
                    results[i].response_harmfulness = ResponseHarmfulness.HARMFUL if response_harmful else ResponseHarmfulness.UNHARMFUL
                    results[i].metadata["response_scores"] = response_scores

                # âœ… Convert Enum to string for JSON serialization
                self.processed_data.append({
                    "input": item,
                    "output": {
                        "prompt_harmfulness": results[i].prompt_harmfulness.name,  # ğŸ”¹ Enum â†’ string
                        "response_harmfulness": results[i].response_harmfulness.name,  # ğŸ”¹ Enum â†’ string
                        "metadata": results[i].metadata
                    }
                })

        except Exception as e:
            print(f"âŒ Error occurred: {e}")

        return results